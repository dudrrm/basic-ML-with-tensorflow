{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear regression\n",
    "---\n",
    "                                                             written by Yang, Soyoung\n",
    "                                                                      2017.06.29 v1.0\n",
    "                                                                      2017.08.09 v1.1\n",
    "                                                                      2017.10.31 v1.2\n",
    "\n",
    "__위젯(widget)__으로 인풋 샘플들과 매개 변수(hyperparameter, ex. 학습률, epoch의 횟수)를 조정하고\n",
    "\n",
    "__텐서플로우(tensorflow)__를 사용해 선형회기의 방법과 과정을 살펴본다.\n",
    "\n",
    "\n",
    "\n",
    "#### STEPS\n",
    "1. import\n",
    "2. widgets - get parameters\n",
    "3. Graphing\n",
    "4. Session run\n",
    "5. Result\n",
    "6. Run with batch step\n",
    "    \n",
    "\n",
    "#### References\n",
    "+ [linear regression by Sung Kim](https://www.youtube.com/watch?v=TxIVr-nk1so&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm&index=6)\n",
    "+ [jupyternote widget lists](http://ipywidgets.readthedocs.io/en/latest/examples/Widget%20List.html)\n",
    "+ [TensorFlow](https://www.tensorflow.org/get_started/get_started)\n",
    "+ [least square regression with train/test set](https://stackoverflow.com/questions/43170017/linear-regression-with-tensorflow)\n",
    "+ [bokeh-io plotting](http://bokeh.pydata.org/en/latest/_images/notebook_interactors.png) : another plotting method. possible to save or crop the plot image.\n",
    "\n",
    "\n",
    "---\n",
    "## 선형회귀 Linear regression \n",
    "\n",
    "<img src=\"./imgs/linreg_line.gif\" style=\"width: 400px;\">\n",
    "- 데이터를 가장 잘 표현하는 선을 긋는 머신러닝 방법이다. 위의 이미지에서 하늘색 선이 linear regression line이다.\n",
    "- 기존에 있던 데이터를 반영하는 연산식, 가설(Hypothesis)을 만든다. 이후 들어오는 정보를 연산식에 넣어 결과값을 도출하는데 이 결과값(result, 예측값)이 목표값(target)과 가깝게 해서, 전체 에러를 줄이며 더 좋은 가설(연산식)을 만든다.\n",
    "- 위의 사진과 같이 복잡한 데이터를 간단하게 표현해서 앞으로를 예측한다. 즉 훈련된 연산식을 통해서 \"좋은 예측값\"을 뽑는 것이 목표이다.\n",
    "\n",
    "### 1) 비용 cost  \n",
    "<img src=\"./imgs/linreg_cost.png\" style=\"width: 700px;\">\n",
    "\n",
    "- Y = AX + B 을 가설, 연산식이라고 보면 A는 Weight, B는 Bias, X는 인풋, Y는 목표값이다. 이상적으로는 AX+b 와 Y가 같아야 하지만 데이터가 많다면 같기 어렵고 항상 차이가 발생한다.\n",
    "- Y'= AX+B, 즉 Y'를 예측값이라고 본다면 Y와 Y'의 차이(error)를 줄이는 것이 linear regression의 목표이다.\n",
    "- 즉 데이터를 잘 표현하는 선을 긋기 위해선 선(예측값, Y')과 각 점들(목표값, Y) 사이의 거리(에러)를 줄여야 한다. [L2-norm](http://mathworld.wolfram.com/L2-Norm.html)으로 각각의 거리값을 더한 전체 에러를 줄이도록 A weight와 B bias를 훈련한다.\n",
    "\n",
    "### 2) 기울기 하강법 Gradient descent  \n",
    "<img src=\"./imgs/linreg_convex.png\" style=\"width: 600px;\">\n",
    "\n",
    "- 에러를 최소화 시킬 때 쓰는 방법이다. Weight와 bias의 원래 값에서 각각 예측값에 미친 영향력만큼(편미분)을 빼 Weight와 Bias를 업데이트 시킨다. Linear regression의 에러는 극솟값(local minimum)이 없는 볼록한 모양(convex)이기 때문에 Gradient Descent를 하면 global minima를 찾을 수 있다.\n",
    "\n",
    "\n",
    "### 3) 정규화 regularization  \n",
    "<img src=\"./imgs/linreg_overfit.png\" style=\"width: 500px;\">\n",
    "<br>\n",
    "<img src=\"./imgs/linreg_reg.png\" style=\"width: 600px;\">\n",
    "\n",
    "- 하지만 에러가 0이 된다고 해서 항상 좋은 것은 아니다. 트레이닝 데이터에만 너무 최적화되어 있다면, 새로운 데이터를 제대로 처리하지 못할 수 있다. 따라서 cost를 구할 때 정규화 식(regularization form)을 넣어서 연산식(Hypothesis)의 차원을 제한한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from ipywidgets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. widget\n",
    "+ Make input samples, controlling noises.\n",
    "+ Decide learning rate and epoch.\n",
    "+ every variables are decided by users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30255f0c3a35400badcae734288ccf59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f312157fa546f0869f4d28e9972473"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4298953b1a8d4294b184776a62dd3799"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## defining widgets\n",
    "# make samples using widgets\n",
    "w_sample_size = widgets.Dropdown(options=[50,100,300,1000],value=100,\\\n",
    "                                 description='sample size : ')\n",
    "w_sample_noises = widgets.FloatSlider(value=0.50, min=0.00, max=2.00,step=0.01,\\\n",
    "                                      description='sample noises : ', readout=True)\n",
    "\n",
    "# make parameters using widgets\n",
    "w_lr = widgets.FloatSlider(min=0.000, max=1.000, step=0.001, value=0.001, \\\n",
    "                           description='learning rate : ', readout=True, readout_format='0.3f')\n",
    "w_epoch = widgets.Dropdown(options=[100,200,500], value=200, description='epoch : ', disabled=False)\n",
    "w_batch_size = widgets.Dropdown(options=[10,50,100],value=50,\\\n",
    "                                 description='batch size : ')\n",
    "\n",
    "# toggle button widget for initialization\n",
    "w_init = widgets.ToggleButton(value=False, description='Initialize', disabled=False,\n",
    "                             button_style='info',icon='check')\n",
    "\n",
    "## function to merge widgets\n",
    "# make data samples by getting parameters from widgets\n",
    "def merge_widget_for_data_plot(sample_size, noises):\n",
    "    print('Sample size is {}, Noises is {}'.format(sample_size, noises))\n",
    "    X = np.linspace(0,10,sample_size)\n",
    "    Y = 2*X + noises*np.random.randn(sample_size)\n",
    "    plt.plot(X,Y,'bo', label='origin data')\n",
    "    plt.show()\n",
    "    return(X, Y, sample_size)\n",
    "\n",
    "def merge_widget_for_model_plot(lr, epoch, batch):\n",
    "    print('Learning rate is {}, Epoch is {}, and Batch size is : {}'.format(lr, epoch,batch))\n",
    "    return(lr, epoch, batch)\n",
    "\n",
    "def initialize(value):\n",
    "    if value: # run when value==true`\n",
    "        w_sample_size.value = 200\n",
    "        w_sample_noises.value = 0.50\n",
    "        w_lr.value=0.001\n",
    "        w_epoch.value=200\n",
    "\n",
    "## interactive \n",
    "merge_widget_data = interactive(merge_widget_for_data_plot, \\\n",
    "                             sample_size=w_sample_size, noises=w_sample_noises)\n",
    "merge_widget_model = interactive(merge_widget_for_model_plot, lr=w_lr, epoch=w_epoch,\\\n",
    "                                batch = w_batch_size)\n",
    "restart = interactive(initialize, value=w_init)\n",
    "\n",
    "## display interactive widgets\n",
    "display(merge_widget_data, merge_widget_model, restart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __get data / parameters from preceded cell__\n",
    "+ __devide data into 2 sets (train set, test set)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr is 0.001, epoch_size is 200, batch is 50\n"
     ]
    }
   ],
   "source": [
    "# defining widget outputs to global variable\n",
    "(input_data, target_data, sample_size) = merge_widget_data.result\n",
    "(lr, epoch_size, batch_size) = merge_widget_model.result\n",
    "\n",
    "print('lr is {}, epoch_size is {}, batch is {}'.format(lr, epoch_size,batch_size)) # check\n",
    "\n",
    "\n",
    "## devide data to 2 sets (for train, tests)\n",
    "# use when sample size is big enoug\n",
    "# to_train_data = round(len(input_data)*0.7)\n",
    "# train_input = input_data[:to_train_data]\n",
    "# train_target = target_data[:to_train_data]\n",
    "# test_input = input_data[to_train_data:]\n",
    "# test_target = target_data[to_train_data:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Make graph of tensorflow\n",
    "+ optimizer can be Gradient Descent optimizer or Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## placeholder for input and target samples\n",
    "X = tf.placeholder(name='input_data', dtype=tf.float32)\n",
    "Y = tf.placeholder(name='target_data', dtype=tf.float32)\n",
    "\n",
    "## if devided train set and test set\n",
    "# X_train = tf.placeholder(name='train_input', dtype=tf.float32)\n",
    "# Y_train = tf.placeholder(name='train_target', dtype=tf.float32)\n",
    "# X_test = tf.placeholder(name='test_input', dtype=tf.float32)\n",
    "# Y_test = tf.placeholder(name='test_target', dtype=tf.float32)\n",
    "\n",
    "## Model parameters\n",
    "w = tf.Variable(name='weight', initial_value=0, dtype=tf.float32)\n",
    "b = tf.Variable(name='bias', initial_value=1, dtype=tf.float32)\n",
    "\n",
    "## Model w*X + b\n",
    "Y_predict = tf.add(tf.multiply(w, X), b)\n",
    "\n",
    "## loss function, sse\n",
    "loss = tf.reduce_sum(tf.square(Y-Y_predict))\n",
    "\n",
    "## optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "trainer = optimizer.minimize(loss)\n",
    "# or\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "## initializer\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Session run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Session.run the graph we made\n",
    "+ stack weight and bias to see training process in 5\n",
    "+ show progress when epoch is times of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "After 0 epochs: total_loss=0.0033, weight=1.8298, bias=1.4279             \n",
      "After 10 epochs: total_loss=0.0028, weight=1.8794, bias=0.9677             \n",
      "After 20 epochs: total_loss=0.0026, weight=1.9110, bias=0.6676             \n",
      "After 30 epochs: total_loss=0.0025, weight=1.9316, bias=0.4720             \n",
      "After 40 epochs: total_loss=0.0024, weight=1.9450, bias=0.3445             \n",
      "After 50 epochs: total_loss=0.0024, weight=1.9537, bias=0.2614             \n",
      "After 60 epochs: total_loss=0.0023, weight=1.9594, bias=0.2073             \n",
      "After 70 epochs: total_loss=0.0023, weight=1.9632, bias=0.1720             \n",
      "After 80 epochs: total_loss=0.0023, weight=1.9656, bias=0.1490             \n",
      "After 90 epochs: total_loss=0.0023, weight=1.9672, bias=0.1340             \n",
      "After 100 epochs: total_loss=0.0023, weight=1.9682, bias=0.1242             \n",
      "After 110 epochs: total_loss=0.0023, weight=1.9689, bias=0.1178             \n",
      "After 120 epochs: total_loss=0.0023, weight=1.9693, bias=0.1137             \n",
      "After 130 epochs: total_loss=0.0023, weight=1.9696, bias=0.1110             \n",
      "After 140 epochs: total_loss=0.0023, weight=1.9698, bias=0.1092             \n",
      "After 150 epochs: total_loss=0.0023, weight=1.9699, bias=0.1081             \n",
      "After 160 epochs: total_loss=0.0023, weight=1.9700, bias=0.1073             \n",
      "After 170 epochs: total_loss=0.0023, weight=1.9700, bias=0.1068             \n",
      "After 180 epochs: total_loss=0.0023, weight=1.9700, bias=0.1065             \n",
      "After 190 epochs: total_loss=0.0023, weight=1.9701, bias=0.1063             \n",
      "\n",
      "Training is DONE.\n"
     ]
    }
   ],
   "source": [
    "# stack weights and biases while trainig\n",
    "w_stack = []\n",
    "b_stack = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize before start\n",
    "    print('Initialized')\n",
    "    # start training\n",
    "    for epoch in range(epoch_size):\n",
    "        total_loss = 0\n",
    "        for step, data in enumerate(zip(input_data, target_data)):\n",
    "            x, y = data # step-th sample in input_data, target_data\n",
    "            _, train_loss = sess.run([trainer, loss], feed_dict={X:x, Y:y})\n",
    "            total_loss += train_loss\n",
    "            current_w, current_b = sess.run([w, b])\n",
    "            w_stack.append(current_w)\n",
    "            b_stack.append(current_b)\n",
    "            total_loss = total_loss/sample_size\n",
    "        if epoch%10==0:\n",
    "            print('After {} epochs: total_loss={:0.4f}, weight={:0.4f}, bias={:0.4f}\\\n",
    "             '.format(epoch, total_loss, current_w, current_b))\n",
    "    print('\\nTraining is DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Control weight and bias from untraind to trained\n",
    "- or Drag third slider through the process step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3384e9abd29545c386e3da26c96bdf23"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget_w_stack = widgets.SelectionSlider(options=w_stack, value=w_stack[0], \\\n",
    "                                         description='w : untrained--->trained ', readout=True)\n",
    "widget_b_stack = widgets.SelectionSlider(options=b_stack, value=b_stack[0], \\\n",
    "                                         description='b : untrained--->trained ',readout=True)\n",
    "\n",
    "def merge_widgets_plot_results(w_value, b_value):\n",
    "    plt.plot(input_data,target_data,'bo', label='origin data')\n",
    "    reg_line = w_value * input_data + b_value\n",
    "    plt.plot(input_data,reg_line, 'r-')\n",
    "    plt.show()\n",
    "\n",
    "show_lsreg = interactive(merge_widgets_plot_results, w_value=widget_w_stack, b_value=widget_b_stack)\n",
    "display(show_lsreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b924719e349240319deba9399f626309"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step_lists = list(range(len(w_stack)))\n",
    "widget_train_process = widgets.SelectionSlider(options=step_lists, value=0,\\\n",
    "                                              description='untrained ---> trained', readout=True)\n",
    "\n",
    "def show_process_following_widgets(step):\n",
    "    w_value = w_stack[step]\n",
    "    b_value = b_stack[step]\n",
    "    plt.plot(input_data,target_data,'bo', label='origin data')\n",
    "    reg_line = w_value * input_data + b_value\n",
    "    plt.plot(input_data,reg_line, 'r-')\n",
    "    plt.show()\n",
    "\n",
    "show_process = interactive(show_process_following_widgets, step=widget_train_process)\n",
    "display(show_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Run with batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) generate batch sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(x, y, batch_size, shuffle=True):\n",
    "    data_size = len(x)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_x = x[shuffle_indices]\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "    else :\n",
    "        shuffled_x = x\n",
    "        shuffled_y = y\n",
    "\n",
    "    num_batches = int((data_size-1)/batch_size)+1\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num+1)*batch_size,data_size)\n",
    "        x_s = shuffled_x[start_index:end_index]\n",
    "        batch_x.append(x_s)\n",
    "        y_s = shuffled_y[start_index:end_index]\n",
    "        batch_y.append(y_s)\n",
    "    return(batch_x, batch_y, num_batches)\n",
    "\n",
    "(batch_input, batch_target, num_batches) = generate_batch(input_data, target_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## placeholder for input and target samples\n",
    "X = tf.placeholder(shape=[None],name='input_data', dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None],name='target_data', dtype=tf.float32)\n",
    "\n",
    "## Model parameters\n",
    "w = tf.Variable(name='weight', initial_value=0, dtype=tf.float32)\n",
    "b = tf.Variable(name='bias', initial_value=1, dtype=tf.float32)\n",
    "\n",
    "## Model w*X + b\n",
    "Y_predict = tf.add(tf.multiply(w,X),b)\n",
    "\n",
    "## loss function, sse\n",
    "loss = tf.reduce_mean(tf.square(Y-Y_predict))\n",
    "\n",
    "## optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "trainer = optimizer.minimize(loss)\n",
    "    # or\n",
    "    # optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "## initializer\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) session.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "After 0 epochs: total_loss=2.0965, weight=0.2364, bias=1.0344             \n",
      "\n",
      "After 10 epochs: total_loss=0.1330, weight=1.4125, bias=1.1996             \n",
      "\n",
      "After 20 epochs: total_loss=0.0179, weight=1.6982, bias=1.2311             \n",
      "\n",
      "After 30 epochs: total_loss=0.0111, weight=1.7686, bias=1.2304             \n",
      "\n",
      "After 40 epochs: total_loss=0.0105, weight=1.7868, bias=1.2220             \n",
      "\n",
      "After 50 epochs: total_loss=0.0104, weight=1.7925, bias=1.2118             \n",
      "\n",
      "After 60 epochs: total_loss=0.0103, weight=1.7951, bias=1.2012             \n",
      "\n",
      "After 70 epochs: total_loss=0.0102, weight=1.7969, bias=1.1906             \n",
      "\n",
      "After 80 epochs: total_loss=0.0100, weight=1.7985, bias=1.1801             \n",
      "\n",
      "After 90 epochs: total_loss=0.0099, weight=1.8001, bias=1.1697             \n",
      "\n",
      "After 100 epochs: total_loss=0.0098, weight=1.8017, bias=1.1594             \n",
      "\n",
      "After 110 epochs: total_loss=0.0097, weight=1.8032, bias=1.1492             \n",
      "\n",
      "After 120 epochs: total_loss=0.0096, weight=1.8047, bias=1.1391             \n",
      "\n",
      "After 130 epochs: total_loss=0.0095, weight=1.8062, bias=1.1291             \n",
      "\n",
      "After 140 epochs: total_loss=0.0094, weight=1.8077, bias=1.1192             \n",
      "\n",
      "After 150 epochs: total_loss=0.0093, weight=1.8092, bias=1.1094             \n",
      "\n",
      "After 160 epochs: total_loss=0.0092, weight=1.8107, bias=1.0997             \n",
      "\n",
      "After 170 epochs: total_loss=0.0091, weight=1.8121, bias=1.0901             \n",
      "\n",
      "After 180 epochs: total_loss=0.0090, weight=1.8135, bias=1.0806             \n",
      "\n",
      "After 190 epochs: total_loss=0.0089, weight=1.8149, bias=1.0712             \n",
      "\n",
      "Training is DONE.\n"
     ]
    }
   ],
   "source": [
    "# stack weights and biases while trainig\n",
    "w_batch_stack = []\n",
    "b_batch_stack = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize before start\n",
    "    print('Initialized')\n",
    "    # start training\n",
    "    for epoch in range(epoch_size):\n",
    "        total_loss = 0\n",
    "        for step, data in enumerate(zip(batch_input, batch_target)):\n",
    "            x_batch, y_batch = data # step-th sample in input_data, target_data\n",
    "            _, train_loss = sess.run([trainer, loss], feed_dict={X:x_batch, Y:y_batch})\n",
    "            input_size = len(x_batch)\n",
    "            total_loss += train_loss/input_size\n",
    "            current_w, current_b = sess.run([w, b])\n",
    "            w_batch_stack.append(current_w)\n",
    "            b_batch_stack.append(current_b)\n",
    "        total_loss = total_loss/num_batches\n",
    "        if epoch%10==0:\n",
    "            print('\\nAfter {} epochs: total_loss={:0.4f}, weight={:0.4f}, bias={:0.4f}\\\n",
    "             '.format(epoch, total_loss, current_w, current_b))\n",
    "    print('\\nTraining is DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4ac2a3dc7743c69fe410c6bcbdbcbf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step_lists = list(range(len(w_batch_stack)))\n",
    "widget_train_process = widgets.SelectionSlider(options=step_lists, value=0,\\\n",
    "                                              description='untrained ---> trained', readout=True)\n",
    "\n",
    "def show_process_following_widgets(step):\n",
    "    w_value = w_batch_stack[step]\n",
    "    b_value = b_batch_stack[step]\n",
    "    plt.plot(input_data,target_data,'bo', label='origin data')\n",
    "    reg_line = w_value * input_data + b_value\n",
    "    plt.plot(input_data,reg_line, 'r-')\n",
    "    plt.show()\n",
    "\n",
    "show_process = interactive(show_process_following_widgets, step=widget_train_process)\n",
    "display(show_process)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
